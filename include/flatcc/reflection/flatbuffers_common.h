#ifndef FLATBUFFERS_COMMON_H
#define FLATBUFFERS_COMMON_H

/* Generated by flatcc 0.1.0 FlatBuffers schema compiler for C by dvide.com */

/* Common FlatBuffers read functionality for C. */

#if __clang__
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wunused-function"
#pragma clang diagnostic ignored "-Wunused-variable"
#endif
/* Include portability layer first if not on compliant C11 platform. */
#ifdef FLATCC_PORTABLE
#include "flatcc/flatcc_portable.h"
#endif
#ifndef UINT8_MAX
#include <stdint.h>
#endif
/*
 * For undetected platforms, provide a custom <endian.h> file in the include path,
 * or include equivalent functionality (the 6 lexxtoh, htolexx functions) before this file.
 */
#if !defined(FLATBUFFERS_LITTLEENDIAN)
#if (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__) ||\
      (defined(__BYTE_ORDER) && __BYTE_ORDER == __LITTLE_ENDIAN) ||\
      (defined(_BYTE_ORDER) && _BYTE_ORDER == _LITTLE_ENDIAN) ||\
    defined(__LITTLE_ENDIAN__) || defined(__ARMEL__) || defined(__THUMBEL__) ||\
    /* for MSVC */ defined(_M_X64) || defined(_M_IX86) || defined(_M_I86) ||\
    defined(__AARCH64EL__) || defined(_MIPSEL) || defined(__MIPSEL) || defined(__MIPSEL__)
#define FLATBUFFERS_LITTLEENDIAN 1
#elif (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__) ||\
      (defined(__BYTE_ORDER) && __BYTE_ORDER == __BIG_ENDIAN) ||\
      (defined(_BYTE_ORDER) && _BYTE_ORDER == _BIG_ENDIAN) ||\
    defined(__BIG_ENDIAN__) || defined(__ARMEB__) || defined(__THUMBEB__) ||\
    defined(__AARCH64EB__) || defined(_MIPSEB) || defined(__MIPSEB) || defined(__MIPSEB__)
#define FLATBUFFERS_LITTLEENDIAN 0
#endif
#endif
#if defined(flatbuffers_is_native_pe)
/* NOP */
#elif !defined(le16toh) && FLATBUFFERS_LITTLEENDIAN
#define le16toh(n) ((uint16_t)(n))
#define le32toh(n) ((uint32_t)(n))
#define le64toh(n) ((uint64_t)(n))
#define htole16(n) ((uint16_t)(n))
#define htole32(n) ((uint32_t)(n))
#define htole64(n) ((uint64_t)(n))
/* pe indicates protocol endian, which for FlatBuffers is little endian. */
#define flatbuffers_is_native_pe() 1
#else
#if defined(OS_FREEBSD) || defined(OS_OPENBSD) || defined(OS_NETBSD) || defined(OS_DRAGONFLYBSD)
#include <sys/types.h>
#include <sys/endian.h>
#else
#include <endian.h>
#endif
#if defined(letoh16) && !defined(le16toh)
#define le16toh letoh16
#define le32toh letoh32
#define le64toh letoh64
#endif
#endif
#ifndef flatbuffers_is_native_pe
#ifdef le16toh
/* This will not automatically define FLATBUFFERS_LITTLEENDIAN, but it will work. */
#define flatbuffers_is_native_pe() (le16toh(1) == 1)
#else
#error "unable to detect endianness - define FLATBUFFERS_LITTLEENDIAN to 1 or 0"
#endif
#endif
#include <assert.h>
/* clang assert.h sometimes fail to do this even with -std=c11 */
#ifndef static_assert
#ifndef FLATBUFFERS_NO_STATIC_ASSERT
#define static_assert(pred, msg) _Static_assert(pred, msg)
#else
#define static_assert(pred, msg)
#endif
#endif
/* Only needed for string search. */
#include <string.h>

#ifndef flatbuffers_types_defined
#define flatbuffers_types_defined

#define flatbuffers_uoffset_t_defined
#define flatbuffers_soffset_t_defined
#define flatbuffers_voffset_t_defined

#define FLATBUFFERS_UOFFSET_MAX UINT32_MAX
#define FLATBUFFERS_SOFFSET_MAX INT32_MAX
#define FLATBUFFERS_SOFFSET_MIN INT32_MIN
#define FLATBUFFERS_VOFFSET_MAX UINT16_MAX

#define FLATBUFFERS_UOFFSET_WIDTH 32
#define FLATBUFFERS_SOFFSET_WIDTH 32
#define FLATBUFFERS_VOFFSET_WIDTH 16

typedef uint32_t flatbuffers_uoffset_t;
typedef int32_t flatbuffers_soffset_t;
typedef uint16_t flatbuffers_voffset_t;

#define FLATBUFFERS_IDENTIFIER_SIZE 4

typedef char flatbuffers_fid_t[FLATBUFFERS_IDENTIFIER_SIZE];

#define FLATBUFFERS_ID_MAX (FLATBUFFERS_VOFFSET_MAX / sizeof(flatbuffers_voffset_t) - 3)
#define FLATBUFFERS_COUNT_MAX(elem_size) ((elem_size) > 0 ? FLATBUFFERS_UOFFSET_MAX/(elem_size) : 0)

#endif /* flatbuffers_types_defined */


/* Helpers to standardize type definitions.
*/#undef le8toh
#define le8toh(n) (n)
#undef htole8
#define htole8(n) (n)
#undef be8toh
#define be8toh(n) (n)
#undef htobe8
#define htobe8(n) (n)
/*
 * Define flatbuffer accessors be little endian.
 * This must match the `flatbuffers_is_native_pe` definition.
 */
/*
 * NOTE: the strict pointer aliasing rule in C makes it non-trival to byteswap
 * floating point values using intrinsic integer based functions like `le32toh`.
 * We cannot cast through pointers as it will generate warnings and potentially
 * violate optimizer assumptions. Cast through union is a safer approach.
 */
union __flatbuffers_fu32 { float f; uint32_t u32; };
static_assert(sizeof(union __flatbuffers_fu32) == 4, "float cast union should have size 4");
union __flatbuffers_du64 { double d; uint64_t u64; };
static_assert(sizeof(union __flatbuffers_du64) == 8, "double cast union should have size 8");

#define __flatbuffers_define_integer_accessors(N, T, W)\
static inline T N ## _cast_from_pe(T v)\
{ return (T) le ## W ## toh((uint ## W ## _t)v); }\
static inline T N ## _cast_to_pe(T v)\
{ return (T) htole ## W((uint ## W ## _t)v); }\
static inline T N ## _read_from_pe(const void *p)\
{ return N ## _cast_from_pe(*(T *)p); }\
static inline T N ## _read_to_pe(const void *p)\
{ return N ## _cast_to_pe(*(T *)p); }\
static inline T N ## _read(const void *p)\
{ return *(T *)p; }
static inline float flatbuffers_float_cast_from_pe(float f)
{ union __flatbuffers_fu32 x; x.f = f; x.u32 = le32toh(x.u32); return x.f; }
static inline double flatbuffers_double_cast_from_pe(double d)
{ union __flatbuffers_du64 x; x.d = d; x.u64 = le64toh(x.u64); return x.d; }
static inline float flatbuffers_float_cast_to_pe(float f)
{ union __flatbuffers_fu32 x; x.f = f; x.u32 = htole32(x.u32); return x.f; }
static inline double flatbuffers_double_cast_to_pe(double d)
{ union __flatbuffers_du64 x; x.d = d; x.u64 = htole64(x.u64); return x.d; }
static inline float flatbuffers_float_read_from_pe(const void *p)
{ union __flatbuffers_fu32 x; x.u32 = le32toh(*(uint32_t *)p); return x.f; }
static inline double flatbuffers_double_read_from_pe(const void *p)
{ union __flatbuffers_du64 x; x.u64 = le64toh(*(uint64_t *)p); return x.d; }
static inline float flatbuffers_float_read_to_pe(const void *p)
{ union __flatbuffers_fu32 x; x.u32 = htole32(*(uint32_t *)p); return x.f; }
static inline double flatbuffers_double_read_to_pe(const void *p)
{ union __flatbuffers_du64 x; x.u64 = htole64(*(uint64_t *)p); return x.d; }
static inline float flatbuffers_float_read(const void *p) { return *(float *)p; }
static inline double flatbuffers_double_read(const void *p) { return *(double *)p; }
/* This allows us to use FLATBUFFERS_UOFFSET_WIDTH for W. */
#define __flatbuffers_PASTE_3(A, B, C) A ## B ## C
#define __flatbuffers_LOAD_PE(W, n) __flatbuffers_PASTE_3(le, W, toh)(n)
#define flatbuffers_load_uoffset(n)\
(flatbuffers_uoffset_t)__flatbuffers_LOAD_PE(FLATBUFFERS_UOFFSET_WIDTH, n)
#define flatbuffers_load_soffset(n)\
(flatbuffers_soffset_t) __flatbuffers_LOAD_PE(FLATBUFFERS_SOFFSET_WIDTH, n)
#define flatbuffers_load_voffset(n)\
(flatbuffers_voffset_t) __flatbuffers_LOAD_PE(FLATBUFFERS_VOFFSET_WIDTH, n)
/****************************************************************
 *  From this point on endianness is abstracted away accessors. *
 *  Protocol endian (pe) may be either little or big endian.    *
 ****************************************************************/
typedef uint8_t flatbuffers_bool_t;
static const flatbuffers_bool_t flatbuffers_true = 1;
static const flatbuffers_bool_t flatbuffers_false = 0;
#define __flatbuffers_read_scalar_at_byteoffset(N, p, o) N ## _read_from_pe((uint8_t *)(p) + (o))
#define __flatbuffers_read_scalar(N, p) N ## _read_from_pe(p)
#define __flatbuffers_read_vt(ID, offset, t)\
flatbuffers_voffset_t offset = 0;\
{\
    assert(t != 0 && "null pointer table access");\
    flatbuffers_voffset_t id = ID;\
    flatbuffers_voffset_t *vt = (flatbuffers_voffset_t *)((uint8_t *)(t) -\
        (flatbuffers_soffset_t)(flatbuffers_load_uoffset(*(flatbuffers_uoffset_t *)(t))));\
    if (flatbuffers_load_voffset(vt[0]) >= sizeof(vt[0]) * (id + 3)) {\
        offset = flatbuffers_load_voffset(vt[id + 2]);\
    }\
}
#define __flatbuffers_field_present(ID, t) { __flatbuffers_read_vt(ID, offset, t) return offset != 0; }
#define __flatbuffers_scalar_field(N, ID, V, t)\
{\
    __flatbuffers_read_vt(ID, offset, t)\
    return offset ? __flatbuffers_read_scalar_at_byteoffset(N, t, offset) : V;\
}
#define __flatbuffers_struct_field(T, ID, t, r)\
{\
    __flatbuffers_read_vt(ID, offset, t)\
    if (offset) {\
        return (T)((uint8_t *)(t) + offset);\
    }\
    assert(!(r) && "required field missing");\
    return 0;\
}
#define __flatbuffers_offset_field(T, ID, t, r, adjust)\
{\
    flatbuffers_uoffset_t *elem;\
    __flatbuffers_read_vt(ID, offset, t)\
    if (offset) {\
        elem = (flatbuffers_uoffset_t *)((uint8_t *)(t) + offset);\
        /* Add sizeof so C api can have raw access past header field. */\
        return (T)((uint8_t *)(elem) + adjust +\
            (flatbuffers_uoffset_t)((flatbuffers_load_uoffset(*elem))));\
    }\
    assert(!(r) && "required field missing");\
    return 0;\
}
#define __flatbuffers_vector_field(T, ID, t, r) __flatbuffers_offset_field(T, ID, t, r, sizeof(flatbuffers_uoffset_t))
#define __flatbuffers_table_field(T, ID, t, r) __flatbuffers_offset_field(T, ID, t, r, 0)
#define __flatbuffers_vec_len(vec)\
{ return (vec) ? flatbuffers_load_uoffset((((flatbuffers_uoffset_t *)(vec))[-1])) : 0; }
#define __flatbuffers_string_len(s) __flatbuffers_vec_len(s)
static inline flatbuffers_uoffset_t flatbuffers_vec_len(const void *vec)
__flatbuffers_vec_len(vec)
#define __flatbuffers_scalar_vec_at(N, vec, i)\
{ assert(flatbuffers_vec_len(vec) > (i) && "index out of range");\
  return __flatbuffers_read_scalar(N, &(vec)[i]); }
#define __flatbuffers_struct_vec_at(vec, i)\
{ assert(flatbuffers_vec_len(vec) > (i) && "index out of range"); return (vec) + (i); }
/* `adjust` skips past the header for string vectors. */
#define __flatbuffers_offset_vec_at(T, vec, i, adjust)\
{ assert(flatbuffers_vec_len(vec) > (i) && "index out of range");\
  const flatbuffers_uoffset_t *elem = (vec) + (i);\
  return (T)((uint8_t *)(elem) + flatbuffers_load_uoffset(*elem) + adjust); }
#define __flatbuffers_define_scalar_vec_len(N) \
static inline flatbuffers_uoffset_t N ## _vec_len(N ##_vec_t vec)\
{ return flatbuffers_vec_len(vec); }
#define __flatbuffers_define_scalar_vec_at(N, T) \
static inline T N ## _vec_at(N ## _vec_t vec, flatbuffers_uoffset_t i)\
__flatbuffers_scalar_vec_at(N, vec, i)
typedef const char *flatbuffers_string_t;
static inline flatbuffers_uoffset_t flatbuffers_string_len(flatbuffers_string_t s)
__flatbuffers_string_len(s)
typedef const flatbuffers_uoffset_t *flatbuffers_string_vec_t;
typedef flatbuffers_uoffset_t *flatbuffers_string_mutable_vec_t;
static inline flatbuffers_uoffset_t flatbuffers_string_vec_len(flatbuffers_string_vec_t vec)
__flatbuffers_vec_len(vec)
static inline flatbuffers_string_t flatbuffers_string_vec_at(flatbuffers_string_vec_t vec, flatbuffers_uoffset_t i)
__flatbuffers_offset_vec_at(flatbuffers_string_t, vec, i, sizeof(vec[0]))
typedef const void *flatbuffers_generic_table_t;
static flatbuffers_uoffset_t flatbuffers_not_found = (flatbuffers_uoffset_t)-1;
#define __flatbuffers_identity(n) (n)
/* Subtraction doesn't work for unsigned types. */
#define __flatbuffers_scalar_cmp(x, y, n) ((x) < (y) ? -1 : (x) > (y))
static inline int __flatbuffers_string_n_cmp(flatbuffers_string_t v, const char *s, size_t n)
{ flatbuffers_uoffset_t nv = flatbuffers_string_len(v); int x = strncmp(v, s, nv < n ? nv : n);
  return x != 0 ? x : nv < n ? -1 : nv > n; }
/* `n` arg unused, but needed by string find macro expansion. */
static inline int __flatbuffers_string_cmp(flatbuffers_string_t v, const char *s, size_t n) { (void)n; return strcmp(v, s); }
/* A = identity if searching scalar vectors rather than key fields. */
/* Returns lowest matching index not_found. */
#define __flatbuffers_find_by_field(A, V, E, L, K, Kn, T, D)\
{ T v; flatbuffers_uoffset_t a = 0, b, m; if (!(b = L(V))) { return flatbuffers_not_found; }\
  --b;\
  while (a < b) {\
    m = a + ((b - a) >> 1);\
    v = A(E(V, m));\
    if ((D(v, (K), (Kn))) < 0) {\
      a = m + 1;\
    } else {\
      b = m;\
    }\
  }\
  if (a == b) {\
    v = A(E(V, a));\
    if (D(v, (K), (Kn)) == 0) {\
       return a;\
    }\
  }\
  return flatbuffers_not_found;\
}
#define __flatbuffers_find_by_scalar_field(A, V, E, L, K, T)\
__flatbuffers_find_by_field(A, V, E, L, K, 0, T, __flatbuffers_scalar_cmp)
#define __flatbuffers_find_by_string_field(A, V, E, L, K)\
__flatbuffers_find_by_field(A, V, E, L, K, 0, flatbuffers_string_t, __flatbuffers_string_cmp)
#define __flatbuffers_find_by_string_n_field(A, V, E, L, K, Kn)\
__flatbuffers_find_by_field(A, V, E, L, K, Kn, flatbuffers_string_t, __flatbuffers_string_n_cmp)
#define __flatbuffers_define_find_by_scalar_field(N, NK, TK)\
static inline flatbuffers_uoffset_t N ## _vec_find_by_ ## NK(N ## _vec_t vec, TK key)\
__flatbuffers_find_by_scalar_field(N ## _ ## NK, vec, N ## _vec_at, N ## _vec_len, key, TK)
#define __flatbuffers_define_scalar_find(N, T)\
static inline flatbuffers_uoffset_t N ## _vec_find(N ## _vec_t vec, T key)\
__flatbuffers_find_by_scalar_field(__flatbuffers_identity, vec, N ## _vec_at, N ## _vec_len, key, T)
#define __flatbuffers_heap_sort(N, X, A, E, L, TK, TE, D, S)\
static inline void __ ## N ## X ## __heap_sift_down(\
        N ## _mutable_vec_t vec, flatbuffers_ ## uoffset_t start, flatbuffers_ ## uoffset_t end)\
{ flatbuffers_uoffset_t child, root; TK v1, v2, vroot;\
  root = start;\
  while ((root << 1) <= end) {\
    child = root << 1;\
    if (child < end) {\
      v1 = A(E(vec, child));\
      v2 = A(E(vec, child + 1));\
      if (D(v1, v2) < 0) {\
        child++;\
      }\
    }\
    vroot = A(E(vec, root));\
    v1 = A(E(vec, child));\
    if (D(vroot, v1) < 0) {\
      S(vec, root, child, TE);\
      root = child;\
    } else {\
      return;\
    }\
  }\
}\
static inline void __ ## N ## X ## __heap_sort(N ## _mutable_vec_t vec)\
{ flatbuffers_uoffset_t start, end, size;\
  size = L(vec); if (size == 0) return; end = size - 1; start = size >> 1;\
  do { __ ## N ## X ## __heap_sift_down(vec, start, end); } while (start--);\
  while (end > 0) { \
    S(vec, 0, end, TE);\
    __ ## N ## X ## __heap_sift_down(vec, 0, --end); } }
#define __flatbuffers_define_sort_by_field(N, NK, TK, TE, D, S)\
  __flatbuffers_heap_sort(N, _sort_by_ ## NK, N ## _ ## NK, N ## _vec_at, N ## _vec_len, TK, TE, D, S)\
static inline void N ## _vec_sort_by_ ## NK(N ## _mutable_vec_t vec)\
{ __ ## N ## _sort_by_ ## NK ## __heap_sort(vec); }
#define __flatbuffers_define_sort(N, TK, TE, D, S)\
__flatbuffers_heap_sort(N, , __flatbuffers_identity, N ## _vec_at, N ## _vec_len, TK, TE, D, S)\
static inline void N ## _vec_sort(N ## _mutable_vec_t vec) { __ ## N ## __heap_sort(vec); }
#define __flatbuffers_scalar_diff(x, y) ((x) < (y) ? -1 : (x) > (y))
#define __flatbuffers_string_diff(x, y) __flatbuffers_string_n_cmp((x), (const char *)(y), flatbuffers_string_len(y))
#define __flatbuffers_scalar_swap(vec, a, b, TE) { TE tmp = vec[b]; vec[b] = vec[a]; vec[a] = tmp; }
#define __flatbuffers_string_swap(vec, a, b, TE)\
{ TE tmp, d; d = (a - b) * sizeof(vec[0]); tmp = vec[b]; vec[b] = vec[a] + d; vec[a] = tmp - d; }
#define __flatbuffers_define_sort_by_scalar_field(N, NK, TK, TE)\
  __flatbuffers_define_sort_by_field(N, NK, TK, TE, __flatbuffers_scalar_diff, __flatbuffers_scalar_swap)
#define __flatbuffers_define_sort_by_string_field(N, NK)\
  __flatbuffers_define_sort_by_field(N, NK, flatbuffers_string_t, flatbuffers_uoffset_t, __flatbuffers_string_diff, __flatbuffers_string_swap)
#define __flatbuffers_define_scalar_sort(N, T) __flatbuffers_define_sort(N, T, T, __flatbuffers_scalar_diff, __flatbuffers_scalar_swap)
#define __flatbuffers_define_string_sort() __flatbuffers_define_sort(flatbuffers_string, flatbuffers_string_t, flatbuffers_uoffset_t, __flatbuffers_string_diff, __flatbuffers_string_swap)
#define __flatbuffers_define_scalar_vector(N, T, W)\
typedef const T *N ## _vec_t;\
typedef T *N ## _mutable_vec_t;\
__flatbuffers_define_scalar_vec_len(N)\
__flatbuffers_define_scalar_vec_at(N, T)\
__flatbuffers_define_scalar_find(N, T)\
\
__flatbuffers_define_scalar_sort(N, T)

#define __flatbuffers_define_integer_type(N, T, W)\
__flatbuffers_define_integer_accessors(N, T, W)\
__flatbuffers_define_scalar_vector(N, T, W)
#define __flatbuffers_define_real_type(N, T, W)\
/* `double` and `float` scalars accessors defined. */\
__flatbuffers_define_scalar_vector(N, T, W)\

__flatbuffers_define_integer_type(flatbuffers_bool, flatbuffers_bool_t, 8)
__flatbuffers_define_integer_type(flatbuffers_uint8, uint8_t, 8)
__flatbuffers_define_integer_type(flatbuffers_int8, int8_t, 8)
__flatbuffers_define_integer_type(flatbuffers_uint16, uint16_t, 16)
__flatbuffers_define_integer_type(flatbuffers_int16, int16_t, 16)
__flatbuffers_define_integer_type(flatbuffers_uint32, uint32_t, 32)
__flatbuffers_define_integer_type(flatbuffers_int32, int32_t, 32)
__flatbuffers_define_integer_type(flatbuffers_uint64, uint64_t, 64)
__flatbuffers_define_integer_type(flatbuffers_int64, int64_t, 64)
__flatbuffers_define_real_type(flatbuffers_float, float, 32)
__flatbuffers_define_real_type(flatbuffers_double, double, 64)
static inline flatbuffers_uoffset_t flatbuffers_string_vec_find(flatbuffers_string_vec_t vec, const char *s)
__flatbuffers_find_by_string_field(__flatbuffers_identity, vec, flatbuffers_string_vec_at, flatbuffers_string_vec_len, s)
static inline flatbuffers_uoffset_t flatbuffers_string_vec_find_n(flatbuffers_string_vec_t vec, const char *s, size_t n)
__flatbuffers_find_by_string_n_field(__flatbuffers_identity, vec, flatbuffers_string_vec_at, flatbuffers_string_vec_len, s, n)
__flatbuffers_define_string_sort()
#define __flatbuffers_struct_scalar_field(t, M, N)\
{ return t ? __flatbuffers_read_scalar(N, &(t->M)) : 0; }
#define __flatbuffers_struct_struct_field(t, M) { return t ? &(t->M) : 0; }
/* If fid is null, the function returns true without testing as buffer is not expected to have any id. */
static inline int flatbuffers_has_identifier(const void *buffer, const char *fid)
{ return fid == 0 || (flatbuffers_load_uoffset(((flatbuffers_uoffset_t *)buffer)[1]) ==
      ((flatbuffers_uoffset_t)fid[0]) + (((flatbuffers_uoffset_t)fid[1]) << 8) +
      (((flatbuffers_uoffset_t)fid[2]) << 16) + (((flatbuffers_uoffset_t)fid[3]) << 24)); }
#define flatbuffers_verify_endian() flatbuffers_has_identifier("\x00\x00\x00\x00" "1234", "1234")
/* Null file identifier accepts anything, otherwise fid should be 4 characters. */
#define __flatbuffers_read_root(T, K, buffer, fid)\
  ((!buffer || !flatbuffers_has_identifier(buffer, fid)) ? 0 :\
  ((T ## _ ## K ## t)(((uint8_t *)buffer) +\
  flatbuffers_load_uoffset(((flatbuffers_uoffset_t *)buffer)[0]))))
#define __flatbuffers_nested_buffer_as_root(C, N, T, K)\
static inline T ## _ ## K ## t C ## _ ## N ## _as_root_with_identifier(C ## _ ## table_t t, const char *fid)\
{ const uint8_t *buffer = C ## _ ## N(t); return __flatbuffers_read_root(T, K, buffer, fid); }\
static inline T ## _ ## K ## t C ## _ ## N ## _as_root(C ## _ ## table_t t)\
{ const char *fid = T ## _identifier;\
  const uint8_t *buffer = C ## _ ## N(t); return __flatbuffers_read_root(T, K, buffer, fid); }
#define __flatbuffers_buffer_as_root(N, K)\
static inline N ## _ ## K ## t N ## _as_root_with_identifier(const void *buffer, const char *fid)\
{ return __flatbuffers_read_root(N, K, buffer, fid); }\
static inline N ## _ ## K ## t N ## _as_root(const void *buffer)\
{ const char *fid = N ## _identifier;\
  return __flatbuffers_read_root(N, K, buffer, fid); }
#define __flatbuffers_struct_as_root(N) __flatbuffers_buffer_as_root(N, struct_)
#define __flatbuffers_table_as_root(N) __flatbuffers_buffer_as_root(N, table_)

#if __clang__
#pragma clang diagnostic pop
#endif
#endif /* FLATBUFFERS_COMMON_H */
